# k8.yml
name: Kubernetes Deployment

on:
  workflow_run:
    workflows: ["Build and Deploy"]  # This refers to the workflow name in main.yml
    types:
      - completed
    branches:
      - master

permissions:
  id-token: write
  contents: read

jobs:
  deploy-to-k8s:
    # Only run if the Build and Deploy workflow succeeded
    if: ${{ github.event.workflow_run.conclusion == 'success' }}
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        role-to-assume: ${{ secrets.AWS_ROLE_TO_ASSUME }}
        aws-region: us-east-1
        role-session-name: GitHubActions-${{ github.run_id }}

    - name: Get workflow outputs
      run: |
        # Get the outputs from the previous workflow
        CLUSTER_NAME=$(curl -s -H "Authorization: token ${{ secrets.GITHUB_TOKEN }}" \
          "https://api.github.com/repos/${{ github.repository }}/actions/runs/${{ github.event.workflow_run.id }}" \
          | jq -r '.outputs.cluster_name')
        ECR_URL=$(curl -s -H "Authorization: token ${{ secrets.GITHUB_TOKEN }}" \
          "https://api.github.com/repos/${{ github.repository }}/actions/runs/${{ github.event.workflow_run.id }}" \
          | jq -r '.outputs.ecr_repository_url')
        echo "CLUSTER_NAME=$CLUSTER_NAME" >> $GITHUB_ENV
        echo "ECR_REGISTRY=$ECR_URL" >> $GITHUB_ENV

    - name: Configure kubectl
      run: |
        aws eks update-kubeconfig --name ${{ env.CLUSTER_NAME }} --region us-east-1

    - name: Deploy to Kubernetes
      env:
        ECR_REGISTRY: ${{ needs.deploy-infrastructure.outputs.ecr_repository_url }}
        IMAGE_TAG: ${{ github.sha }}
        CLUSTER_NAME: ${{ needs.deploy-infrastructure.outputs.cluster_name }}
      run: |
        cd aws_infra
        
        # Enhanced error handling and logging
        set -e  # Exit on any error
        
        echo "Getting cluster information..."
        CLUSTER_INFO=$(aws eks describe-cluster --name $CLUSTER_NAME)
        CLUSTER_STATUS=$(echo $CLUSTER_INFO | jq -r '.cluster.status')
        
        if [ "$CLUSTER_STATUS" != "ACTIVE" ]; then
        echo "Cluster is not active. Status: $CLUSTER_STATUS"
        exit 1
        fi
        
        # Get IAM role for node group
        NODE_ROLE_ARN=$(aws iam list-roles --query "Roles[?contains(RoleName, 'node') && contains(RoleName, '${CLUSTER_NAME}')].Arn" --output text)
        if [ -z "$NODE_ROLE_ARN" ]; then
        echo "Error: Could not find node role for cluster ${CLUSTER_NAME}"
        exit 1
        fi
        
        # Get subnet IDs
        SUBNET_IDS=$(echo $CLUSTER_INFO | jq -r '.cluster.resourcesVpcConfig.subnetIds[]' | tr '\n' ' ')
        if [ -z "$SUBNET_IDS" ]; then
        echo "Error: No subnets found for cluster ${CLUSTER_NAME}"
        exit 1
        fi
        
        # Check for existing node groups
        echo "Checking EKS node groups..."
        NODE_GROUPS=$(aws eks list-nodegroups --cluster-name $CLUSTER_NAME --query 'nodegroups[*]' --output text || echo "")
        
        if [ -z "$NODE_GROUPS" ]; then
        echo "No node groups found. Creating default node group..."
        
        # Create node group with more specific configuration
        aws eks create-nodegroup \
            --cluster-name $CLUSTER_NAME \
            --nodegroup-name default-nodegroup \
            --scaling-config minSize=1,maxSize=3,desiredSize=2 \
            --subnet-ids $SUBNET_IDS \
            --instance-types t3.medium \
            --node-role $NODE_ROLE_ARN \
            --ami-type AL2_x86_64 \
            --remote-access ssh=true \
            --tags Owner=GithubAction,Environment=production
        
        echo "Waiting for nodegroup to be active (this may take several minutes)..."
        aws eks wait nodegroup-active \
            --cluster-name $CLUSTER_NAME \
            --nodegroup-name default-nodegroup
        
        # Verify node group creation
        if ! aws eks describe-nodegroup --cluster-name $CLUSTER_NAME --nodegroup-name default-nodegroup; then
            echo "Failed to create node group"
            exit 1
        fi
        fi
        
        # Wait for nodes to be ready
        echo "Waiting for nodes to be ready..."
        until kubectl get nodes -o name | grep -q .; do
        echo "Waiting for nodes to appear..."
        sleep 10
        done
        
        echo "Cleaning up old deployment resources..."
        kubectl delete deployment massage-website --ignore-not-found=true
        kubectl get pods -l app=massage-website -o name | xargs -r kubectl delete --force --grace-period=0
        kubectl delete rs -l app=massage-website --ignore-not-found=true
        
        echo "Applying manifests..."
        envsubst < k8s/deployment.yml | kubectl apply -f -
        kubectl apply -f k8s/service.yml
        kubectl apply -f k8s/ingress.yml
        
        # Wait for pods to be running
        echo "Waiting for pods to be ready..."
        kubectl wait --for=condition=ready pod -l app=massage-website --timeout=300s
        
        echo "Final deployment status:"
        kubectl get pods,svc,ingress -l app=massage-website